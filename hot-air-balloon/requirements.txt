# Install Pytorch & other libraries
# --no-build-isolation needed for flash-attn
# Preinstall cudatoolkit and torch Before Installing the Requirements File
# then install the remaining packages with the --no-build-isolation flag.
# conda install -c conda-forge cudatoolkit-dev
# pip install torch==2.4.1
# pip install --no-build-isolation -r requirements.txt

# to test:
# # python -c "import torch; major, minor = torch.cuda.get_device_capability(); assert major >= 8, 'Hardware not supported for Flash Attention'; print(f'Compute capability (version): {major}.{minor} â€” your hardware is supported')"
torch==2.4.1
transformers==4.46.3
datasets==3.1.0
trl==0.15.0
tensorboard==2.19.0
python-dotenv==1.0.1
tensorboard== 2.19.0
flash-attn==2.7.4.post1
liger-kernel==0.4.2
setuptools<71.0.0
deepspeed==0.15.4
openai==1.61.1
lm-eval[api]==0.4.5

# Install Hugging Face libraries

accelerate==1.1.1
bitsandbytes==0.44.1

peft==0.13.2
lighteval==0.6.2
hf-transfer==0.1.8
nvitop
matplotlib
